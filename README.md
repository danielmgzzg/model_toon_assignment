# Model Toon Project

## Description

This is a linear regression model to predict the probability of a person to purchase.

## Table of Contents

- [Installation and Usage Guide](#installation-and-usage-guide)
  - [Cloning the Repository](#cloning-the-repository)
  - [Setting Up the Environment](#setting-up-the-environment)
- [How to Use](#how-to-use)
- [Project Functionality](#project-functionality)
- [Makefile Commands](#makefile-commands)
- [Project Structure](#project-structure)

## Installation and Usage Guide

### Cloning the Repository

1. To clone this repository to your local machine, use the following command in your terminal:

```bash
git clone <repository-url>
```

Replace `<repository-url>` with the URL of this repository.

### Setting Up the Environment

1. To set up the virtual environment and install the necessary dependencies, execute the following command:

```bash
make environment
```

This command will create a virtual environment and install all the required packages listed in the `requirements.txt` file.

## How to Use

2. To perform unit testing, use the following command:

```bash
make test
```

3. If you wish to remove the created virtual environment, you can do so by running:

```bash
make clean
```

This command will delete the virtual environment and all its associated files.

## Important Notes

### Azure Account Requirement

To pull data from Azure Blob Storage, a valid Azure account is necessary. A `.env` files is needed with the following:
That contains the service account detailed scoped to Azure Blob Storage for the project.

```bash
GIT_REPO_URL=https://github.com/danielmgzzg/model_toon_assignment.git
AZURE_CLIENT_ID=
AZURE_CLIENT_SECRET=
AZURE_TENANT_ID=
AZURE_STORAGE_CONTAINER_NAME=
AZURE_STORAGE_ACCOUNT_NAME=
AZURE_RESOURCE_GROUP=
KUBEFLOW_HOST=http://localhost:3000/pipeline
REGISTRY=localhost:35147
```

### Minikube

Minikube is used to deploy the pipeline to a local kubeflow instance. To install minikube, use the following command:

```bash
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube
```

### ctlptl

Ctlptl is used to deploy a minikube cluster. To install ctlptl, use the following command:

```bash
brew install tilt-dev/tap/ctlptl
```

Then make sure you have a minikube cluster running:

```bash
make minikube-init

make minikube-start # To start
```

### Tilt

Tilt is used to deploy the pipeline to a local kubeflow instance. The `Tiltfile` is included in the repository. To install tilt, use the following command:

```bash
curl -fsSL https://raw.githubusercontent.com/tilt-dev/tilt/master/scripts/install.sh | bash
```

Then run Tilt

```bash
make tilt
```

### Data Set File

The `data_set.db` file is not included in the repository. It is generated by the `preprocessing.py` script using the `data_set.csv` file. The `data_set.csv` file is retrieved from Azure Blob Storage using Data Version Control (DVC). If you wish to create a `data_set.db` file, you can use the `example_data_set.csv` file which is included in the repository.

### Fetching Different Versions of DVC Tracked Files

If you need to fetch a different version of DVC tracked files, you can use the following command:

```bash
dvc checkout <tag> -- <file_name>
```

In the above command, replace `<tag>` with the tag of the version you want to fetch and replace `<file_name>` with the name of the file you want to fetch. This command will retrieve the specified version of the DVC tracked files.

To fetch the latest version of the DVC tracked files, you can use the following command:

```bash
dvc pull <file_name>
```

Please replace `<file_name>` with the name of the file you want to fetch. This command will pull the latest version of the specified DVC tracked file.

## Project Functionality

This project encompasses a comprehensive machine learning pipeline, offering a range of functionalities from data preprocessing to model evaluation. Here’s a detailed breakdown:

- **DVC Extraction with Azure Blob Storage**: The project utilizes Data Version Control (DVC) to manage and version the machine learning models. This is integrated with Azure Blob Storage, providing a robust and scalable solution for storing and versioning models.

## Makefile Commands

2. **`install`**: This command is used to install the required packages for the project. It reads the `requirements.txt` file and installs all the listed packages.

3. **`data`**: This command adds the configuration and pulls the data required for the project. It uses Data Version Control (DVC) to fetch the data from the current `.dvc` files.

4. **`environment`**: This command sets up the project environment. It creates a virtual environment and installs all the necessary packages.

5. **`build`**: This command builds the Docker image for the project. It uses the `Dockerfile` to build the base image locally and is the parent image.

6. **`tilt`**: This command executes the main script of the project. It's the primary command used to run the project in development and deploys kubeflow, builds and deploys the pipeline on code changes.

7. **`test`**: This command runs the unit tests for the project. It's used to verify that all parts of the project are working correctly.

8. **`clean`**: This command cleans up the project by deleting the virtual environment and all its associated files. It's useful for resetting the project environment.

## Project Structure

```
├── Makefile # Makefile commands
├── README.md # this file
├── notebooks # Jupyter notebooks
│   └── example_usage.ipynb # exercise notebook
├── requirements.txt # project dependencies
├── scripts # shell scripts
│   ├── environment.sh # environment setup script
│   └── setup_project.sh # project setup script
├── setup.py # setup file
├── src # source code
│   ├── pipelines # pipeline files
│   │   ├── compile_pipeline.py
│   │   ├── deploy_pipeline.py
│   │   ├── model_toon_pipeline.py
│   │   ├── model_toon_pipeline.yaml
│   ├── data # data files
│   │   ├── column_info.csv
│   │   ├── column_info.csv.dvc
│   │   ├── data_set.csv
│   │   ├── data_set.csv.dvc
│   │   ├── data_set.db
│   │   ├── marketing_list.csv
│   │   └── plots
│   ├── preprocessing.py # data preprocessing script
│   ├── evaluation.py # evaluation script
│   ├── inference.py # inference script
│   ├── main.py # main script
│   ├── model.py # logistic regression model
│   ├── models # trained models
│   │   ├── logistic_regression_model.pkl # trained logistic regression model fetched from Azure Blob Storage
│   │   └── logistic_regression_model.pkl.dvc # DVC file for logistic regression model
│   ├── training.py
│   └── utils # utility functions
│       ├── custom_exceptions.py
│       ├── logging_config.py
│       └── sqlite_handler.py
├── tests # unit tests
│   ├── test_preprocessing.py
│   ├── test_evaluation.py
│   ├── test_inference.py
│   └── test_model.py
└── venv # virtual environment
```
